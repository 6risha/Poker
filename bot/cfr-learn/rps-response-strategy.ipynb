{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Here the algorithm of **regret matching** for Rock-Paper-Scissors to find the best response strategy to the strategy of the opponent is released"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "40d1c6849500f18e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-21T13:18:33.591965Z",
     "start_time": "2024-04-21T13:18:33.589741Z"
    }
   },
   "id": "4db31a60c1ebb45f",
   "execution_count": 80
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define constants:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5cfaff0d118fb7e9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "ROCK, PAPER, SCISSORS = 0, 1, 2\n",
    "NUM_ACTIONS = 3\n",
    "\n",
    "regretSum = [0.0] * NUM_ACTIONS\n",
    "strategy = [0.0] * NUM_ACTIONS\n",
    "strategySum = [0.0] * NUM_ACTIONS\n",
    "\n",
    "# Choose a strategy for the opponent\n",
    "oppStrategy = [0.4, 0.3, 0.3]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-21T13:18:33.612580Z",
     "start_time": "2024-04-21T13:18:33.609979Z"
    }
   },
   "id": "aa675462a1594a26",
   "execution_count": 81
  },
  {
   "cell_type": "markdown",
   "source": [
    "Function to get current mixed strategy through regret-matching:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f676dea69936d92e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_strategy():\n",
    "    normalizing_sum = 0\n",
    "    for action in range(NUM_ACTIONS):\n",
    "        strategy[action] = max(0, regretSum[action])\n",
    "        normalizing_sum += strategy[action]\n",
    "\n",
    "    for action in range(NUM_ACTIONS):\n",
    "        if normalizing_sum > 0:\n",
    "            strategy[action] /= normalizing_sum\n",
    "        else:\n",
    "            # If we don't regret about anything\n",
    "            strategy[action] = 1.0 / NUM_ACTIONS\n",
    "        strategySum[action] += strategy[action]\n",
    "\n",
    "    return strategy"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-21T13:18:33.616582Z",
     "start_time": "2024-04-21T13:18:33.613708Z"
    }
   },
   "id": "266e145b432c2792",
   "execution_count": 82
  },
  {
   "cell_type": "markdown",
   "source": [
    "Function to get random action according to mixed-strategy distribution:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ed00da5fa25cbfaa"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_action(strategy):\n",
    "    # Next time for this function numpy will be used\n",
    "    r = random.random()\n",
    "    action = 0\n",
    "    cumulativeProbability = 0\n",
    "    while action < NUM_ACTIONS - 1:\n",
    "        cumulativeProbability += strategy[action]\n",
    "        if r < cumulativeProbability:\n",
    "            break\n",
    "        action += 1\n",
    "    return action\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-21T13:18:33.621741Z",
     "start_time": "2024-04-21T13:18:33.619196Z"
    }
   },
   "id": "c66bb9890bb018be",
   "execution_count": 83
  },
  {
   "cell_type": "markdown",
   "source": [
    "Function to get average mixed strategy across all training iterations"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "287b2808940617fd"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_average_strategy():\n",
    "    avg_strategy = [0.0] * NUM_ACTIONS\n",
    "    normalizing_sum = sum(strategySum)\n",
    "    for action in range(NUM_ACTIONS):\n",
    "        if normalizing_sum > 0:\n",
    "            avg_strategy[action] = strategySum[action] / normalizing_sum\n",
    "        else:\n",
    "            avg_strategy[action] = 1.0 / NUM_ACTIONS\n",
    "    return avg_strategy\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-21T13:18:33.626116Z",
     "start_time": "2024-04-21T13:18:33.623357Z"
    }
   },
   "id": "6806b01f8241f766",
   "execution_count": 84
  },
  {
   "cell_type": "markdown",
   "source": [
    "With these building blocks in place, we can now construct our training algorithm:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "812ba32db3706af4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train(iterations):\n",
    "    for _ in range(iterations):\n",
    "\n",
    "        # Get regret-matched mixed strategy actions\n",
    "        strategy = get_strategy()\n",
    "        myAction = get_action(strategy)\n",
    "        otherAction = get_action(oppStrategy)\n",
    "\n",
    "        # Compute action utilities\n",
    "\n",
    "        # Method from guide book is a bit unreadable\n",
    "        # actionUtility = [0.0] * NUM_ACTIONS\n",
    "        # actionUtility[otherAction] = 0\n",
    "        # actionUtility[(otherAction + 1) % NUM_ACTIONS] = 1\n",
    "        # actionUtility[(otherAction - 1) % NUM_ACTIONS] = -1\n",
    "\n",
    "        # We can rewrite it using the matrix of utility (represented in notes)\n",
    "        # myAction - vertical, otherAction - horizontal, RPS accordingly to the order\n",
    "        actionUtility = [[0, -1, 1],\n",
    "                         [1, 0, -1],\n",
    "                         [-1, 1, 0]]\n",
    "\n",
    "        # Accumulate action regrets\n",
    "        for action in range(NUM_ACTIONS):\n",
    "            # In the guide book the following line is used, but with it strategy converges to [0, 1, 0]\n",
    "            # Which means that we always play paper... It doesn't seem to be correct\n",
    "            regretSum[action] += (actionUtility[action][otherAction] - actionUtility[myAction][otherAction])\n",
    "\n",
    "            # But if suppose that we don't add negative regrets to the sum,\n",
    "            # The strategy goes to [0.33, 0.36, 0.31]\n",
    "            # This seems to be more correct\n",
    "            # regretSum[action] += max(0 , actionUtility[action][otherAction] - actionUtility[myAction][otherAction])\n",
    "\n",
    "        # print(regretSum)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-21T13:18:33.631587Z",
     "start_time": "2024-04-21T13:18:33.627587Z"
    }
   },
   "id": "b6139b7d4147fafb",
   "execution_count": 85
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we are ready to run the computation of the response strategy:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ff69efe794f59df4"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0022815968170315994, 0.9933221881942534, 0.004396214988714989]\n"
     ]
    }
   ],
   "source": [
    "train(iterations=10000)\n",
    "print(get_average_strategy())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-21T13:18:33.650880Z",
     "start_time": "2024-04-21T13:18:33.633700Z"
    }
   },
   "id": "4d56d2580699d0ac",
   "execution_count": 86
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Expected utility if we consider wins and losses: 0.099\n",
      " Expected utility if we consider only losses: 0.005\n"
     ]
    }
   ],
   "source": [
    "# Let's compare these two methods: whether we learn on both wins and losses or only on losses\n",
    "actionUtility = [[0, -1, 1],\n",
    "                 [1, 0, -1],\n",
    "                 [-1, 1, 0]]\n",
    "\n",
    "strategyWL = [0.0022815968170315994, 0.9933221881942534, 0.004396214988714989]\n",
    "strategyL = [0.32656612190646006, 0.36233503087561797, 0.31109884721792197]\n",
    "\n",
    "expUtilityWL = 0\n",
    "expUtilityL = 0\n",
    "\n",
    "for i in range(NUM_ACTIONS):\n",
    "    for j in range(NUM_ACTIONS):\n",
    "        expUtilityWL += strategyWL[i] * oppStrategy[j] * actionUtility[i][j]\n",
    "        expUtilityL += strategyL[i] * oppStrategy[j] * actionUtility[i][j]\n",
    "        \n",
    "print(f' Expected utility if we consider wins and losses: {expUtilityWL:0.3f}')\n",
    "print(f' Expected utility if we consider only losses: {expUtilityL:0.3f}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-21T13:18:33.655024Z",
     "start_time": "2024-04-21T13:18:33.652081Z"
    }
   },
   "id": "1760835fb058f72d",
   "execution_count": 87
  },
  {
   "cell_type": "markdown",
   "source": [
    "So we can conclude that it is better to consider wins and losses for regret minimization, because the expected utility will be higher"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "90dc03387e4590b1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
